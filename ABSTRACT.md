In this paper, the authors introduce the LVOS benchmark dataset, comprising 220 videos totaling 421 minutes, representing the first densely annotated long-term Video Object Segmentation (VOS) dataset. The videos within LVOS have an average duration of 1.59 minutes, with each frame meticulously and manually annotated through a semi-automatic annotation pipeline, addressing potential errors in tracking, segmentation, and prediction. The annotation process involves utilizing transfiner for pixel-wise segmentation at 1 FPS, manual marking of initial bounding boxes using MixFormer for propagation, and subsequent refinement with EISeg, correcting approximately 30% of frames on average.

To ensure dataset quality, the authors meticulously select 220 videos from an initial pool of 600 720P-resolution candidate videos, maintaining a balance between video quality and representation. The resulting dataset encompasses 126,280 frames and 156,432 annotations, surpassing the combined size of other datasets. For training, validation, and testing purposes, the videos are partitioned into subsets while preserving distribution and video length characteristics, with 120 videos designated for training, 50 for validation, and 50 for testing. Annotations for training and validation sets are publicly available, fostering the development of VOS methods, while those for the testing set remain private for competition purposes.